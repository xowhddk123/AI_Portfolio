{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CIFAR-100 Classification with resne\n",
    "\n",
    "유튜브 : https://youtu.be/Fh3vxJNoREA  \n",
    "출처 :  https://github.com/heartcored98/Standalone-DeepLearning/blob/master/Lec6/Lab8_CIFAR_100_with_ResNet.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 결과 폴더 만들기\n",
    "# !mkdir results"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import argparse\n",
    "import numpy as np\n",
    "import time\n",
    "from copy import deepcopy # Add Deepcopy for args\n",
    "import seaborn as sns \n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "### parameters\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preparation\n",
    "- torchvision의 CIFAR100 데이터 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "transform = transforms.Compose(\n",
    "    [transforms.ToTensor(),\n",
    "     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "'''\n",
    "RGB 각 채널의 픽셀 값에서 0.5를 뺀 뒤 0.5로 나누어 정규화를 진행합니다.\n",
    "\n",
    "즉, transforms.ToTensor()가 이미지 픽셀 값의 범위를 0 ~ 1 로 조정했으므로,\n",
    "\n",
    "최소값(=-1)은 (0 - 0.5) / 0.5 = -1, 최대값(=1) 은 (1 - 0.5) / 0.5 = 1 로 조정됩니다.\n",
    "\n",
    "결국, 위의 예시를 적용한 결과는 -1 ~ 1 범위로 변환됩니다.\n",
    "'''\n",
    "\n",
    "trainset = torchvision.datasets.CIFAR100(root='./data', train=True,\n",
    "                                        download=True, transform=transform)\n",
    "trainset, valset = torch.utils.data.random_split(trainset, [40000, 10000])\n",
    "testset = torchvision.datasets.CIFAR100(root='./data', train=False,\n",
    "                                       download=True, transform=transform)\n",
    "partition = {'train': trainset, 'val':valset, 'test':testset}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Architecture\n",
    "\n",
    "https://github.com/pytorch/vision/blob/main/torchvision/models/resnet.py\n",
    "\n",
    "\n",
    "### conv3x3 and conv1x1 functions\n",
    "자주 사용하게 될 1x1과 3x3 filter convolutional layer는 필수 파라미터인 in_palnes, out_planes, stride만을 받아 convolutional layer module를 return 해주는 함수를 만들어 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv3x3(in_planes, out_planes, stride=1):\n",
    "    \"\"\"3x3 convolution with padding\"\"\"\n",
    "    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride,\n",
    "                     padding=1, bias=False)\n",
    "    \n",
    "def conv1x1(in_planes, out_planes, stride=1):\n",
    "    \"\"\"1x1 convolution\"\"\"\n",
    "    return nn.Conv2d(in_planes, out_planes, kernel_size=1, stride=stride,\n",
    "                     bias=False) \n",
    "    # 1x1이라 padding 필요없음\n",
    "            "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BasicBlock Module\n",
    "2개의 3x3 convolution layer와 skip connection으로 구성된 BasicBlock module을 구현해봅시다.  \n",
    "![image.png](./basicblock.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BasicBlock(nn.Module):\n",
    "    expansion = 1\n",
    "    \n",
    "    def __init__(self,inplanes, planes, stride=1, downsample=None):\n",
    "        super(BasicBlock, self).__init__()\n",
    "        self.conv1 = conv3x3(inplanes, planes, stride) # 처음에 받는 conv \n",
    "        self.bn1 = nn.BatchNorm2d(planes)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.conv2 = conv3x3(planes, planes)  # 중간에 있는 conv\n",
    "        self.bn2 = nn.BatchNorm2d(planes)\n",
    "        self.downsample = downsample\n",
    "        self.stride = stride\n",
    "        \n",
    "    def forward(self, x):\n",
    "        identity = x  # skip connection에 필요한 identity 함수\n",
    "        \n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "        \n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "        \n",
    "        if self.downsample is not None:  # downsample이 있으면 적용하고 identity 함수에 넣기\n",
    "            identity = self.downsample(x)\n",
    "            \n",
    "        out += identity   # 그림처럼 x를 더함\n",
    "        out = self.relu(out)\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bottleneck Module\n",
    "50개 이상의 layer를 가진 ResNet 구조에서 computational efficiency를 증가시키기 위해 3x3 convolution layer 앞뒤로 1x1 convolution layer를 추가한 bottleneck module을 구현\n",
    "\n",
    "![image.png](./bottleneck.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Bottleneck(nn.Module):\n",
    "    expansion = 4\n",
    "    \n",
    "    def __init__(self, inplanes, planes, stride=1, downsample=None):\n",
    "        super(Bottleneck, self).__init__()\n",
    "        self.conv1 = conv1x1(inplanes, planes)\n",
    "        self.bn1 = nn.BatchNorm2d(planes)\n",
    "        self.conv2 = conv3x3(planes, planes*self.expansion)\n",
    "        self.bn2 = nn.BatchNorm2d(planes)\n",
    "        self.conv3 = conv1x1(planes, planes*self.expansion)\n",
    "        self.bn3 = nn.BatchNorm2d(planes * self.expansion)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.downsample = downsample\n",
    "        self.stride = stride\n",
    "    \n",
    "    def forward(self,x):\n",
    "        identity = x\n",
    "        \n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "        \n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "        out = self.relu(out)\n",
    "        \n",
    "        out = self.conv3(out)\n",
    "        out = self.bn3(out)\n",
    "        \n",
    "        if self.downsample is not None:\n",
    "            identity = self.downsample(x)\n",
    "            \n",
    "        out += identity\n",
    "        out = self.relu(out)\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ResNet Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class ResNet(nn.Module):\n",
    "    \n",
    "    def __init__(self, block, layers, num_classes=1000, zero_init_residual=False):\n",
    "        super(ResNet, self).__init__()\n",
    "        self.inplanes = 64\n",
    "        self.conv1 = nn.Conv2d(3, 64, kernel_size = 7, stride=2, padding=3, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(64)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
    "        self.layer1 = self._make_layer(block, 64, layers[0])\n",
    "        self.layer2 = self._make_layer(block, 128, layers[1], stride=2)\n",
    "        self.layer3 = self._make_layer(block, 256, layers[2], stride=2)\n",
    "        self.layer4 = self._make_layer(block, 512, layers[3], stride=2)\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((1,1))\n",
    "        self.fc = nn.Linear(512*block.expansion, num_classes)\n",
    "        \n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                nn.init.constant_(m.weight, 1)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "                \n",
    "                \n",
    "        if zero_init_residual:\n",
    "            for m in self.modules():\n",
    "                if isinstance(m, Bottleneck):\n",
    "                    nn.init.constant_(m.bn3.weight, 0)\n",
    "                elif isinstance(m, BasicBlock):\n",
    "                    nn.init.constant_(m.bn2.weight, 0)\n",
    "                    \n",
    "    def _make_layer(self, block, planes, blocks, stride=1):\n",
    "        downsample = None \n",
    "        if stride !=1 or self.inplanes != planes*block.expansion:\n",
    "            downsample = nn.Sequential(\n",
    "                conv1x1(self.inplanes, planes*block.expansion, stride),\n",
    "                nn.BatchNorm2d(planes*block.expansion)\n",
    "            )\n",
    "            \n",
    "        layers = []\n",
    "        layers.append(block(self.inplanes, planes, stride, downsample))\n",
    "        self.inplanes = planes*block.expansion\n",
    "        for _ in range(1, blocks):\n",
    "            layers.append(block(self.inplanes, planes))\n",
    "            \n",
    "        return nn.Sequential(*layers)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.maxpool(x)\n",
    "        \n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.layer4(x)\n",
    "        \n",
    "        x = self.avgpool(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.fc(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "# resnet18  구현\n",
    "def resnet18():\n",
    "    model = ResNet(BasicBlock, [2,2,2,2]).to(device)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 6.0107e-01,  4.9231e-02,  2.4208e-01,  2.5600e-01, -3.8055e-02,\n",
      "         -3.6437e-02,  6.4482e-02, -8.6102e-02, -4.6938e-01, -1.3142e-01,\n",
      "          4.4681e-01,  1.1460e+00, -6.3919e-01,  2.9833e-01, -1.5261e-01,\n",
      "          1.9781e-01,  1.5369e-02,  6.4159e-01,  4.8990e-01,  5.8901e-01,\n",
      "          2.5151e-02, -2.0510e-02,  9.3507e-01,  2.6199e-01,  4.1282e-01,\n",
      "          5.6980e-02,  9.7089e-02,  8.0861e-02, -5.0532e-01, -2.3718e-01,\n",
      "          1.6715e-01,  8.1247e-01, -2.3328e-01,  5.8190e-01, -4.6477e-02,\n",
      "          2.2448e-01, -4.3782e-02, -1.7134e-01,  7.3829e-01,  3.5022e-01,\n",
      "         -1.5749e-01,  4.7758e-01,  4.7319e-01, -2.3651e-01, -3.1186e-01,\n",
      "         -5.5259e-01,  1.9218e-01,  4.5288e-01,  4.1871e-01,  9.8440e-02,\n",
      "         -1.6310e-01,  2.6022e-01,  4.6517e-01,  2.1080e-01,  4.7147e-01,\n",
      "         -1.9352e-01,  8.6048e-01, -6.2044e-01,  3.4643e-01, -4.2868e-01,\n",
      "         -5.8247e-01,  6.1556e-01, -1.0031e-01,  3.5204e-01,  1.6389e-01,\n",
      "          2.6657e-01, -4.1921e-01, -3.1141e-01, -2.7567e-01,  6.0375e-01,\n",
      "          5.6192e-02, -5.3628e-02,  3.9510e-01,  7.9740e-01, -8.8207e-01,\n",
      "         -5.4562e-01, -3.6535e-01,  6.0697e-01,  3.2933e-01,  8.0930e-02,\n",
      "          2.1879e-01, -4.7085e-01, -6.0753e-01, -2.0194e-01, -8.2996e-01,\n",
      "          1.0341e-01, -5.3109e-02,  7.2639e-01, -3.8955e-01,  1.0269e+00,\n",
      "         -7.4667e-01,  6.3359e-01,  5.0019e-01,  2.2686e-01,  2.7918e-01,\n",
      "          1.4566e-01,  3.7450e-02,  6.4876e-01,  4.4829e-01, -4.8098e-01,\n",
      "          6.2123e-01, -6.5712e-01,  1.0018e+00,  2.8783e-01,  1.4951e-01,\n",
      "         -1.0760e+00,  5.7499e-02, -3.2322e-01,  5.9202e-01, -8.0439e-01,\n",
      "         -6.1953e-02,  7.6526e-01,  3.8599e-02, -1.5084e-01, -3.1140e-02,\n",
      "         -4.5767e-01, -8.6558e-01, -5.2839e-01,  4.9062e-02,  2.7626e-02,\n",
      "          5.3877e-01,  2.9624e-02,  2.7857e-01,  6.5698e-01,  8.2456e-01,\n",
      "          3.7415e-01, -5.4687e-01, -4.2960e-02, -3.1911e-01,  3.9807e-01,\n",
      "          1.9952e-01, -4.1005e-02, -1.0617e-01, -1.0967e+00,  7.0765e-01,\n",
      "          8.5943e-02,  3.4314e-01,  5.3809e-01,  7.2974e-01,  8.8367e-01,\n",
      "          1.4908e-01, -1.1465e+00,  3.8727e-02, -1.0887e-01, -1.9370e-01,\n",
      "          1.5109e-01,  6.9206e-01, -4.9363e-01, -2.8124e-01, -4.9189e-02,\n",
      "         -3.3980e-01, -6.9859e-02,  8.3643e-01,  3.7354e-01, -3.1234e-01,\n",
      "          2.5230e-01,  3.9531e-01, -1.4929e-01,  2.1199e-01, -2.0989e-01,\n",
      "         -2.9846e-01, -1.8046e-01, -4.6309e-02, -5.0818e-01, -3.1421e-01,\n",
      "          2.9219e-02,  3.6055e-02, -7.8089e-01,  8.7888e-02, -2.2417e-01,\n",
      "         -3.9351e-01, -1.1631e+00,  5.3577e-01, -3.9216e-01,  1.7249e-01,\n",
      "          1.3889e-02,  2.6741e-01,  8.0713e-01,  2.5879e-01, -8.8516e-01,\n",
      "         -8.9588e-02,  3.5258e-01,  1.6165e-01, -3.6832e-01, -3.0445e-01,\n",
      "          2.5201e-01,  8.7342e-01, -8.7978e-01, -6.6543e-01, -2.8908e-01,\n",
      "         -1.5367e+00, -2.3731e-01,  1.3401e-01,  2.5116e-02,  3.6669e-01,\n",
      "          1.0446e+00,  3.7770e-01, -1.8413e-01,  8.2568e-02, -9.6638e-03,\n",
      "          4.6369e-01, -6.2139e-02, -1.3529e-01,  4.0639e-01, -2.0473e-01,\n",
      "          4.5494e-02,  1.0595e-01,  1.4769e-01, -6.7736e-01,  1.9516e-01,\n",
      "          4.5379e-01,  5.2907e-02,  7.5420e-01, -1.1675e+00, -8.6633e-01,\n",
      "          3.5277e-01,  1.3512e+00, -7.2632e-01,  4.7028e-01,  3.2241e-01,\n",
      "          1.5929e-01, -3.0537e-01, -1.1841e+00, -4.0991e-01,  1.2202e+00,\n",
      "          8.7818e-02,  1.2540e+00,  3.1131e-02, -7.9201e-01,  3.5568e-01,\n",
      "         -5.2402e-01,  5.5224e-01, -2.0015e-01, -1.3996e+00, -1.8475e-02,\n",
      "          6.2101e-01,  3.9666e-01,  5.2753e-01, -3.0763e-01, -2.7108e-02,\n",
      "         -1.1173e-01,  1.3843e-01,  7.1562e-01,  2.8400e-02, -2.0480e-02,\n",
      "          3.6635e-02, -6.0500e-01,  1.0623e-01,  2.0136e-02,  5.5183e-01,\n",
      "          2.3622e-01, -3.6869e-01,  4.7265e-01,  6.1310e-01,  2.9309e-01,\n",
      "          5.3772e-01, -1.0687e-01,  4.9299e-01, -1.5011e-01, -3.3663e-01,\n",
      "         -9.7167e-01,  1.3320e-01, -1.6001e-01, -4.2351e-01, -3.6908e-01,\n",
      "         -6.9519e-01,  1.4032e-01,  4.3971e-01, -2.2039e-01, -4.1319e-02,\n",
      "         -3.6429e-01,  2.8506e-01,  5.0407e-01,  1.3693e-01, -2.5719e-01,\n",
      "          7.6851e-01, -1.6320e-01,  1.1356e+00,  6.5715e-01, -9.6634e-02,\n",
      "         -7.0869e-01,  9.2602e-01, -1.9705e-01,  3.4755e-01,  7.4092e-02,\n",
      "         -3.9713e-01,  3.2359e-02, -2.7041e-02,  2.1661e-01, -6.8996e-01,\n",
      "         -7.7594e-01,  5.1352e-01,  8.8134e-02, -2.4551e-01, -9.5491e-02,\n",
      "         -6.4968e-01,  2.3775e-01, -7.2370e-01, -8.9211e-02, -2.2413e-02,\n",
      "         -4.8592e-01, -1.6498e-01, -4.9794e-01, -3.6847e-01, -8.5969e-01,\n",
      "         -3.4627e-01, -3.6835e-01, -3.0773e-01, -3.2672e-01,  1.1701e-02,\n",
      "          7.0875e-01,  4.6724e-01, -7.9665e-01, -8.7015e-02, -1.9614e-01,\n",
      "         -4.7480e-01,  4.2657e-01, -2.8601e-01,  3.1618e-01,  4.5755e-01,\n",
      "         -2.4715e-01,  1.9028e-01, -1.3875e-01, -2.3620e-01,  8.3238e-01,\n",
      "         -2.7269e-02,  8.7270e-02, -6.0680e-01, -1.0165e-01,  6.6647e-01,\n",
      "          4.3952e-01, -2.2395e-01,  2.3799e-02, -6.2606e-02, -4.6258e-01,\n",
      "         -2.4473e-01,  8.5829e-01, -5.0944e-01, -9.3847e-01, -6.6795e-01,\n",
      "         -1.8696e-02,  5.9230e-01,  2.0884e-01,  3.9565e-01, -8.0502e-02,\n",
      "         -3.8765e-01,  1.4193e-01,  2.1153e-01,  7.5473e-02,  1.0605e+00,\n",
      "          2.8592e-02, -8.5229e-01,  3.7103e-02,  1.5489e-01,  4.7068e-01,\n",
      "         -1.8493e-01, -4.2319e-01,  6.9417e-01, -2.3757e-01, -1.4979e-01,\n",
      "          8.6321e-03, -5.9349e-02,  6.0916e-01, -5.4512e-01, -4.6815e-01,\n",
      "          5.7174e-01, -3.5704e-02, -4.3141e-01, -1.7426e-01,  2.0918e-01,\n",
      "         -5.6145e-02, -6.6770e-01, -2.1040e-01,  3.2569e-01,  2.3158e-01,\n",
      "          3.5592e-01,  4.2430e-01,  1.6897e-01, -6.3089e-01, -4.1351e-02,\n",
      "         -1.8322e-01, -1.5291e-01, -9.9241e-02, -5.7796e-01,  5.0657e-02,\n",
      "          6.2705e-01,  1.2984e+00,  2.8261e-01,  4.6516e-01,  3.3794e-01,\n",
      "         -8.4020e-02, -9.7306e-01, -3.4957e-01, -3.3564e-02, -5.1867e-01,\n",
      "         -7.6249e-01, -2.1562e-01, -1.6281e-01,  5.6824e-01, -1.6932e-01,\n",
      "          7.3236e-01, -6.3682e-02, -4.6442e-01,  3.7194e-01, -4.8196e-01,\n",
      "          1.0182e+00, -1.0691e+00,  2.2742e-01,  6.4327e-02,  2.9345e-01,\n",
      "          1.7175e-01,  1.9252e-01, -5.4682e-01,  5.7534e-02,  2.4514e-01,\n",
      "          1.9250e-01,  3.4548e-01,  1.4782e-01,  7.7865e-01,  2.4084e-01,\n",
      "          1.0263e-01, -6.9292e-01, -1.0395e-01,  2.9462e-01, -6.7304e-01,\n",
      "         -3.3237e-02, -7.4689e-02,  6.2796e-02,  4.1486e-01, -4.0204e-02,\n",
      "          4.2348e-01,  3.6467e-01,  5.4519e-01,  6.2157e-02, -1.8380e-01,\n",
      "          4.4604e-01,  5.6678e-01,  8.0925e-01,  1.9349e-01, -5.6272e-02,\n",
      "          2.9507e-01, -3.1943e-01,  1.4297e-02, -6.0731e-01, -1.0445e-01,\n",
      "          7.0855e-01, -1.0078e-01, -7.6496e-02, -3.6824e-01, -1.7446e-01,\n",
      "          4.7571e-01, -2.0388e-01, -1.2859e-01, -1.1324e-02, -2.4609e-01,\n",
      "         -3.1401e-01, -1.0745e-01,  9.9185e-02,  2.9852e-01, -5.1054e-01,\n",
      "         -3.5490e-01, -6.0409e-01, -6.5576e-01, -6.2179e-01, -7.6206e-02,\n",
      "          7.5184e-01,  1.1463e-02,  2.9837e-01,  4.7214e-01, -8.2920e-01,\n",
      "         -2.4301e-01,  4.3274e-01, -2.8564e-01,  2.3762e-01,  1.2190e-01,\n",
      "         -4.9950e-01, -4.8982e-01,  1.1534e+00, -9.0376e-02,  1.3602e-01,\n",
      "         -5.3539e-01, -3.5330e-02, -4.4384e-01,  3.0254e-02,  9.5391e-02,\n",
      "         -5.3776e-01, -9.0810e-01, -4.6562e-02,  2.5049e-01, -7.4771e-01,\n",
      "         -1.5179e-01,  4.6358e-01, -3.3396e-03,  7.9860e-01, -2.2931e-01,\n",
      "          3.0073e-01,  2.5841e-01,  2.0980e-01,  4.3764e-02, -3.5656e-01,\n",
      "         -2.7801e-01,  4.5354e-02, -1.3202e-01,  7.7564e-02,  7.2601e-01,\n",
      "          1.1997e-01,  2.0874e-01,  6.3131e-01, -5.1143e-02, -8.0602e-02,\n",
      "          3.6527e-01,  5.7617e-01,  3.4504e-01, -5.1868e-01,  7.2671e-01,\n",
      "         -1.5907e-01,  4.2408e-01, -5.0417e-01,  4.8646e-01, -1.1346e-01,\n",
      "          8.2145e-01, -4.5121e-01,  7.2419e-02, -1.1268e-01,  1.5989e-01,\n",
      "          5.3615e-01,  1.4443e-01,  5.5995e-01,  3.7501e-01,  3.5713e-01,\n",
      "         -7.2295e-01,  2.4402e-01,  3.2228e-02, -2.5871e-01,  2.3320e-01,\n",
      "         -7.8373e-02, -4.0567e-02, -4.2162e-01, -9.8073e-02, -5.8389e-01,\n",
      "         -4.4470e-01,  1.7499e-01, -2.3558e-01, -5.9757e-01,  4.2000e-02,\n",
      "         -8.9310e-01, -3.1083e-01, -2.7346e-01, -7.6496e-01,  2.2224e-01,\n",
      "         -7.8286e-01,  5.9774e-02, -1.1103e+00, -7.7736e-01,  7.2483e-01,\n",
      "          4.9120e-01, -2.7888e-01,  2.5079e-01,  4.8675e-01,  1.9052e-01,\n",
      "          3.9978e-01, -6.3636e-01,  4.9551e-02,  4.7244e-01, -1.8639e-02,\n",
      "         -1.3002e-01,  9.6232e-02, -5.8737e-02, -3.8025e-01, -2.1928e-01,\n",
      "         -3.4251e-01,  1.1956e-01, -9.3904e-01, -4.3691e-01, -2.2354e-01,\n",
      "          4.7850e-01, -2.0953e-01,  7.7549e-01,  5.0893e-02, -1.6988e-01,\n",
      "          9.0542e-02, -1.2472e-01, -3.5288e-01, -8.7809e-01,  1.1394e-01,\n",
      "         -1.7803e-01, -3.2666e-01,  5.1370e-01,  1.5019e-01, -1.2358e-01,\n",
      "         -2.1038e-01, -6.9520e-01, -1.0124e-03,  7.4896e-02,  6.4267e-01,\n",
      "         -1.2000e-01, -4.9501e-01,  6.0275e-01, -1.8772e-01, -1.0198e-01,\n",
      "          1.8492e-01,  2.2789e-01,  1.1333e+00, -8.5297e-02,  3.9319e-01,\n",
      "          2.6709e-01, -4.1213e-01,  4.5951e-01,  7.4962e-02, -1.0662e-01,\n",
      "         -8.3936e-01, -1.0781e-01,  1.0100e-01, -2.7805e-01,  1.8686e-01,\n",
      "         -5.5418e-01,  6.4734e-01, -2.7317e-01,  1.6952e-01, -5.7425e-01,\n",
      "          2.1556e-01, -3.5478e-01, -1.0395e+00, -1.5271e-02, -9.9235e-01,\n",
      "          5.2398e-01,  5.9603e-01,  4.4273e-02, -5.5331e-01,  3.0395e-01,\n",
      "          9.9658e-01,  1.4919e-01, -7.3992e-01, -1.7474e-01,  1.2967e-01,\n",
      "          2.7162e-01,  2.6357e-01,  5.5654e-02,  1.6669e-01,  1.4412e-01,\n",
      "          5.8274e-01, -3.0177e-01,  6.8053e-01, -6.4261e-01,  8.4812e-02,\n",
      "         -1.9363e-01, -1.1481e-01,  5.8815e-02,  2.4753e-01, -6.0544e-02,\n",
      "         -5.0493e-02, -1.7143e-01,  3.7060e-01, -5.3562e-01,  3.8439e-01,\n",
      "          1.1971e-01, -1.2363e-01, -4.0868e-01,  5.4212e-01,  7.7401e-01,\n",
      "          1.7036e-01,  2.5490e-01, -6.7609e-01, -1.8536e-01, -5.4264e-04,\n",
      "          3.6561e-01, -8.3046e-01, -2.6051e-01,  1.6965e-01, -3.7608e-01,\n",
      "         -5.1189e-01, -4.1627e-01,  3.7269e-01,  4.5251e-01, -1.3598e-01,\n",
      "         -4.1258e-01,  1.3183e-01,  1.8242e-01, -4.1901e-01, -5.7116e-01,\n",
      "          2.5459e-01,  1.0097e-02, -6.3262e-01,  2.5995e-01,  2.2492e-01,\n",
      "          2.1300e-01, -1.0910e-02,  3.9819e-01, -3.8709e-02,  1.5948e-01,\n",
      "          5.0493e-01,  5.2657e-01, -1.1053e-01,  4.8824e-01,  3.0401e-02,\n",
      "          5.3866e-01,  8.6589e-01,  5.1609e-01, -8.5338e-01,  5.3444e-02,\n",
      "          2.3570e-02,  6.6364e-01, -3.0678e-01, -5.3481e-02,  8.4994e-01,\n",
      "          1.6614e-01, -6.8623e-01, -1.6099e-01, -5.7289e-01,  6.8294e-01,\n",
      "          4.4526e-01, -5.0081e-01,  2.8137e-01,  1.1928e-02, -5.0221e-01,\n",
      "         -9.8437e-02, -1.0900e+00, -3.1780e-01, -6.2526e-01, -6.4120e-01,\n",
      "          5.2785e-01, -1.1032e-01, -1.5821e-01,  5.8946e-01, -5.9515e-01,\n",
      "          9.9043e-02,  5.0981e-01,  1.9090e-01, -1.8508e-01,  2.2618e-01,\n",
      "          2.4219e-01,  5.7703e-01, -1.2306e-02, -2.1194e-01, -1.2623e-01,\n",
      "         -4.9652e-01, -8.3341e-01,  1.8151e-01, -8.9555e-03,  1.9267e-01,\n",
      "         -8.9832e-02,  1.6626e-01, -3.4653e-01, -5.9350e-01,  2.7076e-01,\n",
      "          7.8627e-02, -2.9627e-02,  1.8961e-01, -2.3242e-01,  5.2326e-01,\n",
      "         -2.5186e-01, -6.8187e-02,  7.2994e-01, -2.5024e-01, -3.1734e-01,\n",
      "         -2.8756e-01,  1.2548e+00,  1.0672e-01, -3.2896e-01, -2.1884e-01,\n",
      "          5.3522e-02,  7.5644e-02, -4.3301e-01,  7.2291e-02,  1.0442e+00,\n",
      "          3.1692e-02,  1.2017e-01,  4.4191e-01,  3.1043e-01,  2.5371e-01,\n",
      "          4.4408e-01, -5.1095e-01, -8.6067e-01,  6.2585e-02, -4.6089e-01,\n",
      "         -2.8485e-01,  1.8821e-01,  4.6837e-01,  4.1084e-01, -1.0049e-01,\n",
      "          3.1771e-02,  1.5074e-01,  9.9142e-02,  3.2919e-01, -3.8066e-01,\n",
      "         -3.2197e-01,  7.6846e-02, -8.2853e-01, -1.8339e-01, -2.2696e-01,\n",
      "          9.1459e-01,  8.0365e-02, -3.1166e-01, -3.5780e-01,  3.0176e-01,\n",
      "          5.4127e-01,  4.3241e-01,  2.0607e-01,  4.7233e-01, -8.7192e-01,\n",
      "          9.0454e-01, -1.5322e-01, -9.2114e-01, -2.4886e-01,  1.4153e-01,\n",
      "         -2.4968e-01, -6.5942e-01, -4.7235e-01,  5.1373e-01,  9.2812e-02,\n",
      "         -1.7717e-01,  7.4177e-01, -3.7519e-01,  2.7510e-01,  3.5154e-01,\n",
      "          3.9095e-01,  1.8062e-01,  6.6652e-01,  5.2506e-01,  3.5457e-01,\n",
      "         -1.8483e-01, -7.2113e-01, -4.9410e-01,  3.4263e-01,  4.6368e-01,\n",
      "         -1.6250e-01,  3.1839e-01,  1.9527e-01, -5.4957e-02, -1.2616e+00,\n",
      "         -4.2085e-01,  1.9983e-01, -9.5941e-02, -1.7096e-01, -5.6750e-01,\n",
      "         -2.0806e-01,  2.3506e-01,  5.2910e-01, -4.6373e-01, -8.1801e-02,\n",
      "          2.9222e-01, -6.2818e-01, -9.1480e-01,  6.6676e-02,  2.4721e-01,\n",
      "          4.1572e-01,  7.1042e-01, -1.0869e-01, -1.3571e-02,  5.8468e-01,\n",
      "          3.7045e-01, -6.5828e-03, -4.6946e-01, -3.0852e-01, -3.1654e-01,\n",
      "          8.1357e-01,  1.6545e-01, -4.4167e-02, -4.1933e-01,  3.9557e-01,\n",
      "         -1.0514e+00, -1.9310e-01,  3.2385e-01,  1.3182e+00,  6.1103e-01,\n",
      "         -3.3190e-01, -8.1541e-01,  4.0869e-01, -2.8468e-01,  1.3347e-01,\n",
      "          1.5868e-01, -9.1664e-02,  8.4288e-02, -5.5031e-01,  1.0833e+00,\n",
      "         -3.3085e-02, -4.0452e-01, -1.2222e-01,  2.2598e-01, -7.0503e-01,\n",
      "          1.2591e-01, -3.4081e-01,  1.8639e-01, -1.7156e-01, -5.8367e-01,\n",
      "         -4.1575e-01,  1.0081e-01,  1.8859e-01,  8.3198e-02,  7.9989e-01,\n",
      "          6.4450e-01,  1.3668e+00,  3.8226e-01,  4.0895e-02, -1.2559e+00,\n",
      "          7.2828e-01,  9.8112e-01, -2.6655e-01,  2.3562e-01, -3.9733e-01,\n",
      "         -6.8854e-01, -2.6690e-01,  2.9185e-01,  2.6144e-01,  3.7589e-02,\n",
      "          5.2254e-01, -2.4211e-01, -2.5137e-01, -1.4974e-01, -2.6141e-01,\n",
      "          5.0659e-01, -1.7375e-02,  6.2978e-01, -1.5167e-01, -5.4918e-01,\n",
      "         -2.0999e-01, -1.9878e-03, -4.6945e-01, -1.4513e-01,  2.8826e-01,\n",
      "          3.5515e-01, -6.0197e-02, -1.6045e-01, -7.4353e-01,  6.4395e-01,\n",
      "          1.4088e-01, -5.9713e-01,  2.1826e-01,  5.6279e-01,  2.3186e-01,\n",
      "          8.4086e-02, -4.8958e-01,  3.3287e-01, -1.3727e-01, -8.1134e-01,\n",
      "         -2.0247e-01, -4.6435e-01, -7.8042e-01, -1.3988e-02, -3.4058e-01,\n",
      "          5.1444e-01,  7.6623e-01, -4.6980e-01, -3.7613e-02, -7.5294e-01,\n",
      "         -5.7327e-01,  8.3618e-01, -5.5816e-02, -1.0577e+00,  1.0939e-01,\n",
      "         -9.1183e-01,  5.1892e-01,  8.4948e-01, -3.0227e-01, -2.1501e-01,\n",
      "         -8.7595e-02,  2.5744e-01,  2.1183e-02, -3.2247e-01, -2.8602e-01,\n",
      "         -5.9745e-01, -6.7778e-01, -3.9549e-02,  2.2873e-01, -2.3434e-01,\n",
      "         -2.8074e-01, -1.8543e-01,  1.0818e+00, -1.4696e-01, -4.1153e-01,\n",
      "         -4.7755e-01,  8.0379e-03, -3.0805e-01,  4.8614e-01,  1.9795e-03,\n",
      "          2.0087e-01, -3.2380e-01,  7.3241e-01,  3.2560e-01,  2.3865e-01,\n",
      "         -4.2164e-01, -7.9425e-02,  6.7633e-01,  3.6456e-01, -1.3785e-01,\n",
      "         -2.0411e-01, -3.8369e-01, -7.7223e-02, -2.5560e-01, -5.4292e-01,\n",
      "         -6.2333e-02,  1.0045e-01, -9.9437e-01,  2.5638e-02,  2.1784e-01,\n",
      "         -2.8385e-01, -2.7693e-01, -5.5067e-01, -8.8403e-02,  1.0796e-01]],\n",
      "       grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision.models as models\n",
    "\n",
    "# Instantiate ResNet-18 model\n",
    "model = resnet18()\n",
    "\n",
    "# Load image\n",
    "image = torch.randn(1, 3, 224, 224)\n",
    "\n",
    "# Pass image through model\n",
    "output = model(image)\n",
    "\n",
    "# Print output\n",
    "print(output)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train, Valid, Test and Experiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(net):\n",
    "    trainloader = torch.utils.data.DataLoader(trainset,\n",
    "                                              batch_size=1,\n",
    "                                              shuffle=True, num_workers=2)\n",
    "    \n",
    "    net.train()\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optim = torch.optim.Adam(net.parameters(), lr=0.005)\n",
    "    \n",
    "    correct = 0\n",
    "    total = 0\n",
    "    train_loss = 0.0\n",
    "    for i, data in enumerate(trainloader, 0):\n",
    "        optim.zero_grad() # grad 초기화\n",
    "        \n",
    "        # input\n",
    "        inputs, labels = data\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "        outputs = net(inputs)\n",
    "        \n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optim.step()\n",
    "        \n",
    "        train_loss += loss.item()\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "        \n",
    "    train_loss = train_loss /len(trainloader)\n",
    "    train_acc = 100* correct / total\n",
    "    return net, train_loss, train_acc    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(net):\n",
    "    valloader = torch.utils.data.DataLoader(valset, \n",
    "                                            batch_size=1, \n",
    "                                            shuffle=False, num_workers=2)\n",
    "    net.eval()\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    val_loss = 0 \n",
    "    with torch.no_grad():\n",
    "        for data in valloader:\n",
    "            images, labels = data\n",
    "            images = images.cuda()\n",
    "            labels = labels.cuda()\n",
    "            outputs = net(images)\n",
    "\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            val_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "        val_loss = val_loss / len(valloader)\n",
    "        val_acc = 100 * correct / total\n",
    "    return val_loss, val_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(net):\n",
    "    testloader = torch.utils.data.DataLoader(testset, \n",
    "                                             batch_size=1, \n",
    "                                             shuffle=False, num_workers=2)\n",
    "    net.eval()\n",
    "    \n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for data in testloader:\n",
    "            images, labels = data\n",
    "            images = images.cuda()\n",
    "            labels = labels.cuda()\n",
    "\n",
    "            outputs = net(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "        test_acc = 100 * correct / total\n",
    "    return test_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Expected more than 1 value per channel when training, got input size torch.Size([1, 512, 1, 1])",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\xowhd\\Desktop\\personal\\01.study\\06.tjkim\\01.Resnet\\Resnet.ipynb Cell 23\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/xowhd/Desktop/personal/01.study/06.tjkim/01.Resnet/Resnet.ipynb#X34sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m train(resnet18())\n",
      "\u001b[1;32mc:\\Users\\xowhd\\Desktop\\personal\\01.study\\06.tjkim\\01.Resnet\\Resnet.ipynb Cell 23\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(net)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/xowhd/Desktop/personal/01.study/06.tjkim/01.Resnet/Resnet.ipynb#X34sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m inputs \u001b[39m=\u001b[39m inputs\u001b[39m.\u001b[39mto(device)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/xowhd/Desktop/personal/01.study/06.tjkim/01.Resnet/Resnet.ipynb#X34sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m labels \u001b[39m=\u001b[39m labels\u001b[39m.\u001b[39mto(device)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/xowhd/Desktop/personal/01.study/06.tjkim/01.Resnet/Resnet.ipynb#X34sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m outputs \u001b[39m=\u001b[39m net(inputs)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/xowhd/Desktop/personal/01.study/06.tjkim/01.Resnet/Resnet.ipynb#X34sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m loss \u001b[39m=\u001b[39m criterion(outputs, labels)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/xowhd/Desktop/personal/01.study/06.tjkim/01.Resnet/Resnet.ipynb#X34sZmlsZQ%3D%3D?line=22'>23</a>\u001b[0m loss\u001b[39m.\u001b[39mbackward()\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\torch\\nn\\modules\\module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "\u001b[1;32mc:\\Users\\xowhd\\Desktop\\personal\\01.study\\06.tjkim\\01.Resnet\\Resnet.ipynb Cell 23\u001b[0m in \u001b[0;36mResNet.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/xowhd/Desktop/personal/01.study/06.tjkim/01.Resnet/Resnet.ipynb#X34sZmlsZQ%3D%3D?line=54'>55</a>\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlayer2(x)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/xowhd/Desktop/personal/01.study/06.tjkim/01.Resnet/Resnet.ipynb#X34sZmlsZQ%3D%3D?line=55'>56</a>\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlayer3(x)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/xowhd/Desktop/personal/01.study/06.tjkim/01.Resnet/Resnet.ipynb#X34sZmlsZQ%3D%3D?line=56'>57</a>\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlayer4(x)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/xowhd/Desktop/personal/01.study/06.tjkim/01.Resnet/Resnet.ipynb#X34sZmlsZQ%3D%3D?line=58'>59</a>\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mavgpool(x)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/xowhd/Desktop/personal/01.study/06.tjkim/01.Resnet/Resnet.ipynb#X34sZmlsZQ%3D%3D?line=59'>60</a>\u001b[0m x \u001b[39m=\u001b[39m x\u001b[39m.\u001b[39mview(x\u001b[39m.\u001b[39msize(\u001b[39m0\u001b[39m), \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\torch\\nn\\modules\\module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\torch\\nn\\modules\\container.py:204\u001b[0m, in \u001b[0;36mSequential.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    202\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m):\n\u001b[0;32m    203\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m:\n\u001b[1;32m--> 204\u001b[0m         \u001b[39minput\u001b[39m \u001b[39m=\u001b[39m module(\u001b[39minput\u001b[39;49m)\n\u001b[0;32m    205\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39minput\u001b[39m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\torch\\nn\\modules\\module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "\u001b[1;32mc:\\Users\\xowhd\\Desktop\\personal\\01.study\\06.tjkim\\01.Resnet\\Resnet.ipynb Cell 23\u001b[0m in \u001b[0;36mBasicBlock.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/xowhd/Desktop/personal/01.study/06.tjkim/01.Resnet/Resnet.ipynb#X34sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m identity \u001b[39m=\u001b[39m x  \u001b[39m# skip connection에 필요한 identity 함수\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/xowhd/Desktop/personal/01.study/06.tjkim/01.Resnet/Resnet.ipynb#X34sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m out \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconv1(x)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/xowhd/Desktop/personal/01.study/06.tjkim/01.Resnet/Resnet.ipynb#X34sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m out \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbn1(out)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/xowhd/Desktop/personal/01.study/06.tjkim/01.Resnet/Resnet.ipynb#X34sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m out \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrelu(out)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/xowhd/Desktop/personal/01.study/06.tjkim/01.Resnet/Resnet.ipynb#X34sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m out \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconv2(out)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\torch\\nn\\modules\\module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\torch\\nn\\modules\\batchnorm.py:171\u001b[0m, in \u001b[0;36m_BatchNorm.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    164\u001b[0m     bn_training \u001b[39m=\u001b[39m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrunning_mean \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m) \u001b[39mand\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrunning_var \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m)\n\u001b[0;32m    166\u001b[0m \u001b[39mr\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    167\u001b[0m \u001b[39mBuffers are only updated if they are to be tracked and we are in training mode. Thus they only need to be\u001b[39;00m\n\u001b[0;32m    168\u001b[0m \u001b[39mpassed when the update should occur (i.e. in training mode when they are tracked), or when buffer stats are\u001b[39;00m\n\u001b[0;32m    169\u001b[0m \u001b[39mused for normalization (i.e. in eval mode when buffers are not None).\u001b[39;00m\n\u001b[0;32m    170\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m--> 171\u001b[0m \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mbatch_norm(\n\u001b[0;32m    172\u001b[0m     \u001b[39minput\u001b[39;49m,\n\u001b[0;32m    173\u001b[0m     \u001b[39m# If buffers are not to be tracked, ensure that they won't be updated\u001b[39;49;00m\n\u001b[0;32m    174\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrunning_mean\n\u001b[0;32m    175\u001b[0m     \u001b[39mif\u001b[39;49;00m \u001b[39mnot\u001b[39;49;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtraining \u001b[39mor\u001b[39;49;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrack_running_stats\n\u001b[0;32m    176\u001b[0m     \u001b[39melse\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m,\n\u001b[0;32m    177\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrunning_var \u001b[39mif\u001b[39;49;00m \u001b[39mnot\u001b[39;49;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtraining \u001b[39mor\u001b[39;49;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrack_running_stats \u001b[39melse\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m,\n\u001b[0;32m    178\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight,\n\u001b[0;32m    179\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias,\n\u001b[0;32m    180\u001b[0m     bn_training,\n\u001b[0;32m    181\u001b[0m     exponential_average_factor,\n\u001b[0;32m    182\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49meps,\n\u001b[0;32m    183\u001b[0m )\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\torch\\nn\\functional.py:2448\u001b[0m, in \u001b[0;36mbatch_norm\u001b[1;34m(input, running_mean, running_var, weight, bias, training, momentum, eps)\u001b[0m\n\u001b[0;32m   2435\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m   2436\u001b[0m         batch_norm,\n\u001b[0;32m   2437\u001b[0m         (\u001b[39minput\u001b[39m, running_mean, running_var, weight, bias),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   2445\u001b[0m         eps\u001b[39m=\u001b[39meps,\n\u001b[0;32m   2446\u001b[0m     )\n\u001b[0;32m   2447\u001b[0m \u001b[39mif\u001b[39;00m training:\n\u001b[1;32m-> 2448\u001b[0m     _verify_batch_size(\u001b[39minput\u001b[39;49m\u001b[39m.\u001b[39;49msize())\n\u001b[0;32m   2450\u001b[0m \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39mbatch_norm(\n\u001b[0;32m   2451\u001b[0m     \u001b[39minput\u001b[39m, weight, bias, running_mean, running_var, training, momentum, eps, torch\u001b[39m.\u001b[39mbackends\u001b[39m.\u001b[39mcudnn\u001b[39m.\u001b[39menabled\n\u001b[0;32m   2452\u001b[0m )\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\torch\\nn\\functional.py:2416\u001b[0m, in \u001b[0;36m_verify_batch_size\u001b[1;34m(size)\u001b[0m\n\u001b[0;32m   2414\u001b[0m     size_prods \u001b[39m*\u001b[39m\u001b[39m=\u001b[39m size[i \u001b[39m+\u001b[39m \u001b[39m2\u001b[39m]\n\u001b[0;32m   2415\u001b[0m \u001b[39mif\u001b[39;00m size_prods \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[1;32m-> 2416\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mExpected more than 1 value per channel when training, got input size \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(size))\n",
      "\u001b[1;31mValueError\u001b[0m: Expected more than 1 value per channel when training, got input size torch.Size([1, 512, 1, 1])"
     ]
    }
   ],
   "source": [
    "train(resnet18())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[-0.1529, -0.0667, -0.0588,  ..., -0.5137, -0.3882, -0.4353],\n",
       "          [-0.1137, -0.0118,  0.0353,  ..., -0.6078, -0.5686, -0.6000],\n",
       "          [-0.1451, -0.0353, -0.0745,  ..., -0.7098, -0.7333, -0.7490],\n",
       "          ...,\n",
       "          [ 0.2941,  0.3020,  0.2471,  ...,  0.0902,  0.0588,  0.0353],\n",
       "          [ 0.2392,  0.2941,  0.3020,  ...,  0.1529,  0.1529,  0.1059],\n",
       "          [-0.0431, -0.0431,  0.0431,  ...,  0.1294,  0.1843,  0.1608]],\n",
       " \n",
       "         [[-0.0275, -0.0118, -0.0275,  ..., -0.3490, -0.2235, -0.2706],\n",
       "          [-0.0431,  0.0039,  0.0275,  ..., -0.4667, -0.4275, -0.4667],\n",
       "          [-0.1216, -0.0353, -0.0902,  ..., -0.5922, -0.6157, -0.6471],\n",
       "          ...,\n",
       "          [ 0.2392,  0.2392,  0.1686,  ...,  0.0196, -0.0118, -0.0353],\n",
       "          [ 0.2235,  0.2471,  0.2157,  ...,  0.0510,  0.0588,  0.0196],\n",
       "          [-0.0275, -0.0745, -0.0431,  ...,  0.0275,  0.0824,  0.0667]],\n",
       " \n",
       "         [[-0.5059, -0.4275, -0.4353,  ..., -0.8118, -0.6784, -0.7255],\n",
       "          [-0.4824, -0.3804, -0.3569,  ..., -0.8510, -0.7961, -0.8275],\n",
       "          [-0.4824, -0.3647, -0.4353,  ..., -0.8667, -0.8745, -0.8902],\n",
       "          ...,\n",
       "          [-0.0196,  0.0196, -0.0275,  ..., -0.1843, -0.2314, -0.2784],\n",
       "          [-0.0980, -0.0196, -0.0275,  ..., -0.1529, -0.1608, -0.2078],\n",
       "          [-0.3804, -0.3725, -0.3098,  ..., -0.1922, -0.1294, -0.1451]]]),\n",
       " 43)"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainset.__getitem__(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "26de051ba29f2982a8de78e945f0abaf191376122a1563185a90213a26c5da77"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
